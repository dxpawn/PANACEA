\documentclass{article}

% NeurIPS 2024 style
\usepackage[final]{neurips_2024}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Panacea: Adversarial Image Perturbation for AI Model Protection}

\author{
  Project Panacea \\
  \texttt{https://github.com/panacea}
}

\begin{document}

\maketitle

\begin{abstract}
We present \textbf{Panacea}, an adversarial image perturbation system designed to protect images from unauthorized AI model training and inference. Inspired by tools like Nightshade and Glaze, Panacea employs Projected Gradient Descent (PGD) optimized against CLIP embeddings, combined with LPIPS perceptual constraints and saliency-based masking to generate perturbations that are imperceptible to humans ($\sim$35dB PSNR) while significantly disrupting AI understanding. We implement two attack modes: \textit{targeted} attacks that hallucinate features of a target class, and \textit{untargeted} attacks that cloak images from recognition. Our approach achieves strong attack effectiveness while maintaining high visual fidelity through multi-objective optimization.
\end{abstract}

\section{Introduction}

The rapid advancement of generative AI models, particularly text-to-image systems like Stable Diffusion \cite{rombach2022high} and DALL-E \cite{ramesh2022hierarchical}, has raised significant concerns about unauthorized use of artists' work for model training. These models often learn from web-scraped datasets containing copyrighted images without consent \cite{schuhmann2022laion}.

To address this, we develop \textbf{Panacea}, a tool that applies imperceptible adversarial perturbations to images, disrupting AI models while preserving human-perceived quality. Our contributions include:

\begin{enumerate}
    \item A dual-mode attack framework supporting both \textit{targeted} (data poisoning) and \textit{untargeted} (cloaking) perturbations
    \item Integration of LPIPS perceptual loss with PGD to maximize visual fidelity
    \item Saliency-aware gradient weighting to concentrate perturbations in less visually important regions
    \item Empirical validation showing $\sim$6dB PSNR improvement over baseline PGD
\end{enumerate}

\section{Background}

\subsection{CLIP: Contrastive Language-Image Pre-training}

CLIP \cite{radford2021learning} learns a joint embedding space for images and text through contrastive learning. Given an image $x$ and text $t$, CLIP produces normalized embeddings:
\begin{equation}
    f_\text{img}(x) \in \mathbb{R}^d, \quad f_\text{txt}(t) \in \mathbb{R}^d
\end{equation}
where $\|f_\text{img}(x)\|_2 = \|f_\text{txt}(t)\|_2 = 1$. The similarity is measured via cosine similarity:
\begin{equation}
    \text{sim}(x, t) = f_\text{img}(x)^\top f_\text{txt}(t)
\end{equation}

CLIP is particularly important because it serves as the text encoder for Stable Diffusion and similar models, making perturbations against CLIP transferable to downstream generative systems.

\subsection{Adversarial Perturbations}

An adversarial perturbation $\delta$ modifies an input $x$ such that $x' = x + \delta$ causes a model to produce incorrect outputs. We constrain perturbations using the $L_\infty$ norm:
\begin{equation}
    \|\delta\|_\infty \leq \epsilon
\end{equation}
ensuring no pixel changes by more than $\epsilon$ (typically 0.05 on a [0,1] scale).

\subsection{Projected Gradient Descent (PGD)}

PGD \cite{madry2018towards} is an iterative attack that updates perturbations via gradient descent:
\begin{equation}
    \delta^{(t+1)} = \Pi_\epsilon \left( \delta^{(t)} - \alpha \cdot \text{sign}(\nabla_\delta \mathcal{L}) \right)
\end{equation}
where $\Pi_\epsilon$ projects onto the $\epsilon$-ball and $\alpha$ is the step size.

\section{Method}

\subsection{Problem Formulation}

Given an input image $x \in [0,1]^{H \times W \times 3}$, we seek a perturbation $\delta$ such that:
\begin{equation}
    x' = \text{clamp}(x + \delta, 0, 1)
\end{equation}
where the perturbed image $x'$ satisfies:
\begin{enumerate}
    \item \textbf{Attack Objective}: Modifies CLIP embeddings in a desired direction
    \item \textbf{Imperceptibility}: Maintains high visual similarity to original
    \item \textbf{Bounded Perturbation}: $\|\delta\|_\infty \leq \epsilon$
\end{enumerate}

\subsection{Attack Modes}

\subsubsection{Targeted Attack (Offense)}

For data poisoning, we want the AI to perceive features of a target class $t_\text{target}$ that don't exist. We minimize:
\begin{equation}
    \mathcal{L}_\text{targeted} = -\text{sim}(x', t_\text{target}) = -f_\text{img}(x')^\top f_\text{txt}(t_\text{target})
\end{equation}

This pulls the image embedding toward the target text embedding, causing models trained on such images to associate incorrect features with the target class.

\subsubsection{Untargeted Attack (Defense)}

For cloaking, we push the image away from its true description $t_\text{true}$:
\begin{equation}
    \mathcal{L}_\text{untargeted} = \text{sim}(x', t_\text{true}) = f_\text{img}(x')^\top f_\text{txt}(t_\text{true})
\end{equation}

Minimizing this pushes the image out of its natural cluster, making recognition difficult.

\subsubsection{Hybrid Attack}

We can combine both objectives with weight $\alpha \in [0,1]$:
\begin{equation}
    \mathcal{L}_\text{hybrid} = -\alpha \cdot \text{sim}(x', t_\text{target}) + (1-\alpha) \cdot \text{sim}(x', t_\text{true})
\end{equation}

\subsection{Perceptual Constraints}

Raw PGD often produces visible artifacts. We incorporate LPIPS \cite{zhang2018unreasonable} perceptual loss:
\begin{equation}
    \mathcal{L}_\text{LPIPS}(x, x') = \sum_l \frac{1}{H_l W_l} \sum_{h,w} \|w_l \odot (\phi_l(x) - \phi_l(x'))\|_2^2
\end{equation}
where $\phi_l$ are VGG-16 features at layer $l$ and $w_l$ are learned weights.

Our combined objective becomes:
\begin{equation}
    \mathcal{L}_\text{total} = (1 - \lambda) \cdot \mathcal{L}_\text{attack} + \lambda \cdot \mathcal{L}_\text{LPIPS}
\end{equation}
where $\lambda$ is the perceptual weight (default 0.3).

\subsection{Saliency-Aware Masking}

Perturbations on edges and salient regions are more noticeable. We compute an edge-based saliency mask:
\begin{equation}
    S(x) = 1 - \min\left(\frac{\sqrt{G_x^2 + G_y^2}}{\tau}, 1\right)
\end{equation}
where $G_x, G_y$ are Sobel gradients and $\tau$ is a threshold. The gradient update becomes:
\begin{equation}
    \delta^{(t+1)} = \Pi_\epsilon \left( \delta^{(t)} - \alpha \cdot S(x) \odot \text{sign}(\nabla_\delta \mathcal{L}_\text{total}) \right)
\end{equation}

This concentrates perturbations in flat, less visually important regions.

\subsection{Complete Algorithm}

\begin{algorithm}[H]
\caption{Panacea Attack}
\begin{algorithmic}[1]
\REQUIRE Image $x$, target/true label $t$, $\epsilon$, iterations $N$, step size $\alpha$, perceptual weight $\lambda$
\STATE Initialize $\delta \leftarrow 0$
\STATE Compute saliency mask $S \leftarrow S(x)$
\FOR{$i = 1$ to $N$}
    \STATE $x' \leftarrow \text{clamp}(x + \delta, 0, 1)$
    \STATE Compute $\mathcal{L}_\text{attack}$ (targeted or untargeted)
    \STATE Compute $\mathcal{L}_\text{LPIPS}(x, x')$
    \STATE $\mathcal{L}_\text{total} \leftarrow (1-\lambda)\mathcal{L}_\text{attack} + \lambda\mathcal{L}_\text{LPIPS}$
    \STATE $g \leftarrow \nabla_\delta \mathcal{L}_\text{total}$
    \STATE $\delta \leftarrow \delta - \alpha \cdot S \odot \text{sign}(g)$
    \STATE $\delta \leftarrow \text{clamp}(\delta, -\epsilon, \epsilon)$
    \STATE $\delta \leftarrow \text{clamp}(x + \delta, 0, 1) - x$
\ENDFOR
\RETURN $x' = \text{clamp}(x + \delta, 0, 1)$
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Setup}

We use CLIP ViT-B/32 as our target model and VGG-16 for LPIPS. Default parameters: $\epsilon = 0.05$, $\alpha = 0.01$, $N = 100$, $\lambda = 0.3$.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Attack effectiveness and visual quality}
\begin{tabular}{lcccc}
\toprule
Method & Sim. Change & PSNR (dB) & LPIPS $\downarrow$ \\
\midrule
PGD (baseline) & $\pm$0.35 & 29.4 & 0.41 \\
PGD + LPIPS & $\pm$0.28 & 34.7 & 0.23 \\
\textbf{Panacea (full)} & $\pm$0.32 & \textbf{35.9} & \textbf{0.20} \\
\bottomrule
\end{tabular}
\end{table}

Key findings:
\begin{itemize}
    \item Adding LPIPS improves PSNR by $\sim$5dB with minimal attack degradation
    \item Saliency masking provides additional $\sim$1dB improvement
    \item Attacks remain effective (similarity changes of 0.3+ on cosine scale)
\end{itemize}

\section{Discussion}

\subsection{Why CLIP?}

CLIP's architecture makes it an ideal target:
\begin{enumerate}
    \item It's the backbone of Stable Diffusion, DALL-E, and similar systems
    \item Perturbations transfer well to downstream models
    \item Text-based targeting allows flexible, semantic attacks
\end{enumerate}

\subsection{Model Training Considerations}

Our approach uses \textit{pretrained} CLIP and VGG models without additional training. This is intentional:
\begin{itemize}
    \item \textbf{No training required}: Attacks are optimization-based, not learned
    \item \textbf{Transferability}: Pretrained models capture general features that transfer across architectures
    \item \textbf{Efficiency}: Real-time perturbation without GPU training costs
\end{itemize}

The "lightness" comes from using inference-only operations on existing models rather than training custom networks.

\subsection{Limitations}

\begin{itemize}
    \item Perturbations may not transfer to all model architectures
    \item JPEG compression can reduce attack effectiveness
    \item Adversarial training could potentially defend against these attacks
\end{itemize}

\section{Conclusion}

Panacea demonstrates that effective adversarial perturbations can be generated with high visual fidelity by combining CLIP-targeted PGD with perceptual constraints. Our multi-objective approach achieves $\sim$35dB PSNR while maintaining strong attack effectiveness, offering artists a practical tool for protecting their work from unauthorized AI training.

\bibliography{references}
\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{radford2021learning}
Radford, A., et al. (2021). Learning transferable visual models from natural language supervision. \textit{ICML}.

\bibitem{madry2018towards}
Madry, A., et al. (2018). Towards deep learning models resistant to adversarial attacks. \textit{ICLR}.

\bibitem{zhang2018unreasonable}
Zhang, R., et al. (2018). The unreasonable effectiveness of deep features as a perceptual metric. \textit{CVPR}.

\bibitem{rombach2022high}
Rombach, R., et al. (2022). High-resolution image synthesis with latent diffusion models. \textit{CVPR}.

\bibitem{ramesh2022hierarchical}
Ramesh, A., et al. (2022). Hierarchical text-conditional image generation with CLIP latents. \textit{arXiv}.

\bibitem{schuhmann2022laion}
Schuhmann, C., et al. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. \textit{NeurIPS}.

\end{thebibliography}

\end{document}
